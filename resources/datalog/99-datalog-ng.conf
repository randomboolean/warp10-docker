///////////////////////////////////////////////////////////////////////////////
// Datalog Manager
///////////////////////////////////////////////////////////////////////////////

//
// Datalog NG is enabled by specifying a Datalog Manager class name in the datalog.manager configuration key.
// For the time being, the only supported class is io.warp10.standalone.datalog.FileBasedDatalogManager.
//
datalog.manager=io.warp10.standalone.datalog.FileBasedDatalogManager

//
// Existing directory where the log files should be created. This can be a local or an HDFS directory.
//
datalog.manager.dir=${standalone.home}/datalog-ng

//
// Maximum size (in bytes) for log files, any file growing past that limit will be closed and a new one will be initiated.
//
#datalog.manager.maxsize=134217728

//
// Maximum time after which a log file will be closed, in ms.
//
#datalog.manager.maxtime=600000

//
// Time (in ms) after which log files will be purged. Use 0 to disable automatic purge.
//
#datalog.manager.purge=0

//
// Boolean flag indicating whether or not to sync each even write to the underlying filesystem.
// Setting this to true ensures greater durability of log data in case of failure but has a small performance impact.
//
#datalog.manager.syncall=false

//
// Boolean flag indicating whether or not the log files should be compressed. Set it to true only if your system is really tight on space.
//
#datalog.manager.compress=false

//
// Id attached to all records originating on this node. This id MUST be unique across the nodes in the datalog setup.
//
#datalog.manager.id=

//
// Comma separated list of the only ids whose records SHOULD be forwarded and logged.
//
#datalog.manager.forward

//
// Comma separated list of ids whose records SHOULD NOT be forwarded and hence logged.
//
#datalog.manager.noforward


///////////////////////////////////////////////////////////////////////////////
// Datalog Feeder
///////////////////////////////////////////////////////////////////////////////

//
// Name of macro used to validate consumers. See below for details.
//
datalog.feeder.checkmacro=datalog/feeder.checkauieauieauie

//
// Host/address that the feeder should listen on.
//
#datalog.feeder.host=127.0.0.1

//
// Port the feeder should listen on.
//
#datalog.feeder.port=3465

//
// Maximum number of consurrent consumers.
//
#datalog.feeder.maxclients=2

//
// Maximum size of TCP backlog for connections from consumers.
//
#datalog.feeder.backlog=2

//
// Boolean flag indicating whether or not to encrypt all messages between the feeder and consumers.
//
#datalog.feeder.encrypt=false

//
// Unique id of the feeder, should match that of the Datalog Manager provided in datalog.manager.id.
//
datalog.feeder.id=${datalog.manager.id}

//
// Private key of the feeder
//
#datalog.feeder.ecc.private

//
// Public key of the feeder
//
#datalog.feeder.ecc.public

//
// Maximum time (in ms) to wait for messages from a consumer.
//
#datalog.feeder.timeout=300000

//
// Maximum size in bytes of individual records. MUST be coherent with Warp 10 configuration for maximum encoder size.
//
#datalog.feeder.maxsize=1048576

//
// Maximum size in bytes of unacknowledges messages.
//
#datalog.feeder.maxinfligt=1000000




///////////////////////////////////////////////////////////////////////////////
// Datalog consumer
///////////////////////////////////////////////////////////////////////////////

//
// Name of Datalog Consumer class, for now only io.warp10.standalone.datalog.TCPDatalogConsumer is supported
//
#datalog.consumer.class= io.warp10.standalone.datalog.TCPDatalogConsumer

//
// Host/address of the feeder the consumer should contact.
//
#datalog.consumer.feeder.host=127.0.0.1

//
// Port of the feeder to contact.
//
#datalog.consumer.feeder.port=3465

//
// Expected public key of the feeder. See the note on cryptographics keys below.
//
#datalog.consumer.feeder.ecc.public

//
// Comma separated list of shards to request from the feeder.
// Each shard has the form modulus:remainder where modulus and remainder are positive integers less or equal to 2^32 - 1.
//
#datalog.consumer.feeder.shards

//
// Number of bits to shift the GTS id for determining the shard id.
// Integer between -128 (shifting to the left) and 128 (shifting to the right).
//
#datalog.consumer.feeder.shardshift=48

//
// Comma separated list of shards to accept. Can be combined with datalog.consumer.feeder.shardshift. Syntax of shards is identical.
//
#datalog.consumer.shards

//
// Number of bits to shift the GTS id for determining the shard id. Integer between -128 (shifting to the left) and 128 (shifting to the right).
//
#datalog.consumer.shardshift=48

//
// Name of filtering macro. See below.
//
#datalog.consumer.macro

//
// Boolean flag indicating whether or not to call the filtering macro with the actual data (true) or just the metadata of the GTS (false).
//
#datalog.consumer.macro.data=false

//
// Unique id of the consumer.
//
#datalog.consumer.id

//
// Private key of the consumer. See the note on cryptographics keys below.
//
#datalog.consumer.ecc.private

//
// Public key of the consumer. See the note on cryptographics keys below.
//
#datalog.consumer.ecc.public

//
// Comma separated list of ids whose records should be ignored.
//
#datalog.consumer.excluded

//
// Path to the local file where consuming offsets are stored for this feeder/consumer combination.
//
#datalog.consumer.offsetfile

//
// Time (in ms) between writing of offsets. 0 means write offset as soon as a block of records were successfully applied.
//
#datalog.consumer.offsetdelay=0


#datalog.consumer=
